{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"labeledTrainData.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"testData.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwordswords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    # use the NLTK tokenizer to spilt the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            \n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "for review in train[\"review\"]:\n",
    "    sentences+=review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795872"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-03-02 20:47:17,900 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2018-03-02 20:47:17,920 : INFO : collecting all words and their counts\n",
      "2018-03-02 20:47:17,921 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-02 20:47:18,060 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-02 20:47:18,185 : INFO : PROGRESS: at sentence #20000, processed 451582 words, keeping 24944 word types\n",
      "2018-03-02 20:47:18,298 : INFO : PROGRESS: at sentence #30000, processed 670632 words, keeping 30023 word types\n",
      "2018-03-02 20:47:18,434 : INFO : PROGRESS: at sentence #40000, processed 896478 words, keeping 34329 word types\n",
      "2018-03-02 20:47:18,616 : INFO : PROGRESS: at sentence #50000, processed 1115469 words, keeping 37741 word types\n",
      "2018-03-02 20:47:18,739 : INFO : PROGRESS: at sentence #60000, processed 1336692 words, keeping 40702 word types\n",
      "2018-03-02 20:47:18,872 : INFO : PROGRESS: at sentence #70000, processed 1559365 words, keeping 43300 word types\n",
      "2018-03-02 20:47:18,964 : INFO : PROGRESS: at sentence #80000, processed 1778623 words, keeping 45699 word types\n",
      "2018-03-02 20:47:19,044 : INFO : PROGRESS: at sentence #90000, processed 2002603 words, keeping 48113 word types\n",
      "2018-03-02 20:47:19,114 : INFO : PROGRESS: at sentence #100000, processed 2224101 words, keeping 50180 word types\n",
      "2018-03-02 20:47:19,190 : INFO : PROGRESS: at sentence #110000, processed 2442894 words, keeping 52050 word types\n",
      "2018-03-02 20:47:19,271 : INFO : PROGRESS: at sentence #120000, processed 2665092 words, keeping 54089 word types\n",
      "2018-03-02 20:47:19,362 : INFO : PROGRESS: at sentence #130000, processed 2890948 words, keeping 55829 word types\n",
      "2018-03-02 20:47:19,477 : INFO : PROGRESS: at sentence #140000, processed 3103390 words, keeping 57318 word types\n",
      "2018-03-02 20:47:19,625 : INFO : PROGRESS: at sentence #150000, processed 3328823 words, keeping 59031 word types\n",
      "2018-03-02 20:47:19,717 : INFO : PROGRESS: at sentence #160000, processed 3550557 words, keeping 60569 word types\n",
      "2018-03-02 20:47:19,809 : INFO : PROGRESS: at sentence #170000, processed 3773286 words, keeping 62032 word types\n",
      "2018-03-02 20:47:19,885 : INFO : PROGRESS: at sentence #180000, processed 3994239 words, keeping 63473 word types\n",
      "2018-03-02 20:47:19,964 : INFO : PROGRESS: at sentence #190000, processed 4219119 words, keeping 64766 word types\n",
      "2018-03-02 20:47:20,041 : INFO : PROGRESS: at sentence #200000, processed 4443600 words, keeping 66056 word types\n",
      "2018-03-02 20:47:20,127 : INFO : PROGRESS: at sentence #210000, processed 4663901 words, keeping 67353 word types\n",
      "2018-03-02 20:47:20,200 : INFO : PROGRESS: at sentence #220000, processed 4888988 words, keeping 68632 word types\n",
      "2018-03-02 20:47:20,296 : INFO : PROGRESS: at sentence #230000, processed 5111142 words, keeping 69909 word types\n",
      "2018-03-02 20:47:20,389 : INFO : PROGRESS: at sentence #240000, processed 5338157 words, keeping 71133 word types\n",
      "2018-03-02 20:47:20,472 : INFO : PROGRESS: at sentence #250000, processed 5552543 words, keeping 72322 word types\n",
      "2018-03-02 20:47:20,543 : INFO : PROGRESS: at sentence #260000, processed 5772532 words, keeping 73454 word types\n",
      "2018-03-02 20:47:20,623 : INFO : PROGRESS: at sentence #270000, processed 5992302 words, keeping 74713 word types\n",
      "2018-03-02 20:47:20,695 : INFO : PROGRESS: at sentence #280000, processed 6218589 words, keeping 76300 word types\n",
      "2018-03-02 20:47:20,779 : INFO : PROGRESS: at sentence #290000, processed 6441501 words, keeping 77782 word types\n",
      "2018-03-02 20:47:20,865 : INFO : PROGRESS: at sentence #300000, processed 6666056 words, keeping 79128 word types\n",
      "2018-03-02 20:47:20,941 : INFO : PROGRESS: at sentence #310000, processed 6890439 words, keeping 80423 word types\n",
      "2018-03-02 20:47:21,040 : INFO : PROGRESS: at sentence #320000, processed 7117442 words, keeping 81782 word types\n",
      "2018-03-02 20:47:21,130 : INFO : PROGRESS: at sentence #330000, processed 7339075 words, keeping 82995 word types\n",
      "2018-03-02 20:47:21,238 : INFO : PROGRESS: at sentence #340000, processed 7568395 words, keeping 84249 word types\n",
      "2018-03-02 20:47:21,344 : INFO : PROGRESS: at sentence #350000, processed 7791752 words, keeping 85404 word types\n",
      "2018-03-02 20:47:21,478 : INFO : PROGRESS: at sentence #360000, processed 8011384 words, keeping 86565 word types\n",
      "2018-03-02 20:47:21,599 : INFO : PROGRESS: at sentence #370000, processed 8239070 words, keeping 87662 word types\n",
      "2018-03-02 20:47:21,694 : INFO : PROGRESS: at sentence #380000, processed 8464887 words, keeping 88840 word types\n",
      "2018-03-02 20:47:21,812 : INFO : PROGRESS: at sentence #390000, processed 8693660 words, keeping 89878 word types\n",
      "2018-03-02 20:47:21,901 : INFO : PROGRESS: at sentence #400000, processed 8917611 words, keeping 90882 word types\n",
      "2018-03-02 20:47:21,975 : INFO : PROGRESS: at sentence #410000, processed 9138559 words, keeping 91859 word types\n",
      "2018-03-02 20:47:22,061 : INFO : PROGRESS: at sentence #420000, processed 9358876 words, keeping 92882 word types\n",
      "2018-03-02 20:47:22,137 : INFO : PROGRESS: at sentence #430000, processed 9587270 words, keeping 93912 word types\n",
      "2018-03-02 20:47:22,218 : INFO : PROGRESS: at sentence #440000, processed 9813090 words, keeping 94858 word types\n",
      "2018-03-02 20:47:22,293 : INFO : PROGRESS: at sentence #450000, processed 10037669 words, keeping 95999 word types\n",
      "2018-03-02 20:47:22,379 : INFO : PROGRESS: at sentence #460000, processed 10270891 words, keeping 97068 word types\n",
      "2018-03-02 20:47:22,479 : INFO : PROGRESS: at sentence #470000, processed 10497516 words, keeping 97892 word types\n",
      "2018-03-02 20:47:22,575 : INFO : PROGRESS: at sentence #480000, processed 10718677 words, keeping 98822 word types\n",
      "2018-03-02 20:47:22,666 : INFO : PROGRESS: at sentence #490000, processed 10945340 words, keeping 99846 word types\n",
      "2018-03-02 20:47:22,747 : INFO : PROGRESS: at sentence #500000, processed 11167018 words, keeping 100743 word types\n",
      "2018-03-02 20:47:22,822 : INFO : PROGRESS: at sentence #510000, processed 11392468 words, keeping 101680 word types\n",
      "2018-03-02 20:47:22,899 : INFO : PROGRESS: at sentence #520000, processed 11615357 words, keeping 102564 word types\n",
      "2018-03-02 20:47:22,979 : INFO : PROGRESS: at sentence #530000, processed 11840724 words, keeping 103382 word types\n",
      "2018-03-02 20:47:23,057 : INFO : PROGRESS: at sentence #540000, processed 12064929 words, keeping 104239 word types\n",
      "2018-03-02 20:47:23,142 : INFO : PROGRESS: at sentence #550000, processed 12290277 words, keeping 105107 word types\n",
      "2018-03-02 20:47:23,235 : INFO : PROGRESS: at sentence #560000, processed 12511689 words, keeping 105982 word types\n",
      "2018-03-02 20:47:23,352 : INFO : PROGRESS: at sentence #570000, processed 12739997 words, keeping 106765 word types\n",
      "2018-03-02 20:47:23,427 : INFO : PROGRESS: at sentence #580000, processed 12961661 words, keeping 107628 word types\n",
      "2018-03-02 20:47:23,511 : INFO : PROGRESS: at sentence #590000, processed 13187373 words, keeping 108479 word types\n",
      "2018-03-02 20:47:23,587 : INFO : PROGRESS: at sentence #600000, processed 13409391 words, keeping 109195 word types\n",
      "2018-03-02 20:47:23,661 : INFO : PROGRESS: at sentence #610000, processed 13631453 words, keeping 110066 word types\n",
      "2018-03-02 20:47:23,730 : INFO : PROGRESS: at sentence #620000, processed 13856992 words, keeping 110810 word types\n",
      "2018-03-02 20:47:23,808 : INFO : PROGRESS: at sentence #630000, processed 14080109 words, keeping 111581 word types\n",
      "2018-03-02 20:47:23,887 : INFO : PROGRESS: at sentence #640000, processed 14301626 words, keeping 112397 word types\n",
      "2018-03-02 20:47:24,037 : INFO : PROGRESS: at sentence #650000, processed 14528096 words, keeping 113175 word types\n",
      "2018-03-02 20:47:24,186 : INFO : PROGRESS: at sentence #660000, processed 14749998 words, keeping 113902 word types\n",
      "2018-03-02 20:47:24,311 : INFO : PROGRESS: at sentence #670000, processed 14974475 words, keeping 114623 word types\n",
      "2018-03-02 20:47:24,439 : INFO : PROGRESS: at sentence #680000, processed 15199010 words, keeping 115339 word types\n",
      "2018-03-02 20:47:24,568 : INFO : PROGRESS: at sentence #690000, processed 15421656 words, keeping 116109 word types\n",
      "2018-03-02 20:47:24,708 : INFO : PROGRESS: at sentence #700000, processed 15650148 words, keeping 116922 word types\n",
      "2018-03-02 20:47:24,800 : INFO : PROGRESS: at sentence #710000, processed 15871849 words, keeping 117568 word types\n",
      "2018-03-02 20:47:24,867 : INFO : PROGRESS: at sentence #720000, processed 16098974 words, keeping 118203 word types\n",
      "2018-03-02 20:47:24,953 : INFO : PROGRESS: at sentence #730000, processed 16323039 words, keeping 118927 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-02 20:47:25,021 : INFO : PROGRESS: at sentence #740000, processed 16546073 words, keeping 119639 word types\n",
      "2018-03-02 20:47:25,105 : INFO : PROGRESS: at sentence #750000, processed 16763375 words, keeping 120274 word types\n",
      "2018-03-02 20:47:25,181 : INFO : PROGRESS: at sentence #760000, processed 16982901 words, keeping 120901 word types\n",
      "2018-03-02 20:47:25,260 : INFO : PROGRESS: at sentence #770000, processed 17209327 words, keeping 121680 word types\n",
      "2018-03-02 20:47:25,337 : INFO : PROGRESS: at sentence #780000, processed 17440408 words, keeping 122381 word types\n",
      "2018-03-02 20:47:25,419 : INFO : PROGRESS: at sentence #790000, processed 17666965 words, keeping 123048 word types\n",
      "2018-03-02 20:47:25,470 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795872 sentences\n",
      "2018-03-02 20:47:25,471 : INFO : Loading a fresh vocabulary\n",
      "2018-03-02 20:47:25,589 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2018-03-02 20:47:25,590 : INFO : min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
      "2018-03-02 20:47:25,689 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2018-03-02 20:47:25,700 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2018-03-02 20:47:25,701 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
      "2018-03-02 20:47:25,707 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2018-03-02 20:47:25,823 : INFO : resetting layer weights\n",
      "2018-03-02 20:47:26,188 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-03-02 20:47:27,204 : INFO : PROGRESS: at 0.83% examples, 526437 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:28,207 : INFO : PROGRESS: at 1.42% examples, 448688 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:47:29,240 : INFO : PROGRESS: at 2.15% examples, 449032 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:47:30,247 : INFO : PROGRESS: at 2.80% examples, 438111 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:31,269 : INFO : PROGRESS: at 3.48% examples, 434412 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:32,288 : INFO : PROGRESS: at 4.20% examples, 435600 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:33,290 : INFO : PROGRESS: at 5.00% examples, 446677 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:34,295 : INFO : PROGRESS: at 5.78% examples, 452050 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:35,299 : INFO : PROGRESS: at 6.53% examples, 453949 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:36,314 : INFO : PROGRESS: at 7.17% examples, 448714 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:37,325 : INFO : PROGRESS: at 7.83% examples, 445898 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:38,333 : INFO : PROGRESS: at 8.46% examples, 442466 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:39,347 : INFO : PROGRESS: at 9.14% examples, 440926 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:47:40,358 : INFO : PROGRESS: at 9.73% examples, 436242 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:41,374 : INFO : PROGRESS: at 10.42% examples, 436282 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:42,374 : INFO : PROGRESS: at 10.99% examples, 431821 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:43,378 : INFO : PROGRESS: at 11.62% examples, 430314 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:47:44,381 : INFO : PROGRESS: at 12.30% examples, 430580 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:45,388 : INFO : PROGRESS: at 12.93% examples, 428833 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-02 20:47:46,406 : INFO : PROGRESS: at 13.60% examples, 428463 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:47,421 : INFO : PROGRESS: at 14.26% examples, 427849 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-02 20:47:48,435 : INFO : PROGRESS: at 14.87% examples, 426035 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:49,443 : INFO : PROGRESS: at 15.55% examples, 426017 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:50,467 : INFO : PROGRESS: at 16.28% examples, 427176 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:51,468 : INFO : PROGRESS: at 16.97% examples, 427814 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:52,470 : INFO : PROGRESS: at 17.63% examples, 427557 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:53,484 : INFO : PROGRESS: at 18.35% examples, 428716 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:54,495 : INFO : PROGRESS: at 19.07% examples, 429319 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:55,511 : INFO : PROGRESS: at 19.73% examples, 429079 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:56,526 : INFO : PROGRESS: at 20.54% examples, 431695 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:47:57,555 : INFO : PROGRESS: at 21.36% examples, 434157 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:58,565 : INFO : PROGRESS: at 22.12% examples, 435415 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:47:59,572 : INFO : PROGRESS: at 22.86% examples, 436229 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:00,580 : INFO : PROGRESS: at 23.64% examples, 437793 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:01,610 : INFO : PROGRESS: at 24.40% examples, 438598 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:02,616 : INFO : PROGRESS: at 25.08% examples, 438460 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:03,624 : INFO : PROGRESS: at 25.85% examples, 439638 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:04,645 : INFO : PROGRESS: at 26.75% examples, 442647 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:48:05,663 : INFO : PROGRESS: at 27.54% examples, 444090 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:06,670 : INFO : PROGRESS: at 28.26% examples, 444352 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:07,671 : INFO : PROGRESS: at 29.06% examples, 446046 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:08,688 : INFO : PROGRESS: at 29.84% examples, 447126 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:09,710 : INFO : PROGRESS: at 30.50% examples, 446318 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:10,722 : INFO : PROGRESS: at 31.13% examples, 445165 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:11,723 : INFO : PROGRESS: at 31.80% examples, 445105 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:12,724 : INFO : PROGRESS: at 32.41% examples, 443835 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-02 20:48:13,728 : INFO : PROGRESS: at 33.00% examples, 442432 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:14,728 : INFO : PROGRESS: at 33.62% examples, 441411 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:15,756 : INFO : PROGRESS: at 34.49% examples, 443521 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:48:16,770 : INFO : PROGRESS: at 35.26% examples, 444248 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:17,806 : INFO : PROGRESS: at 36.01% examples, 444606 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:18,805 : INFO : PROGRESS: at 36.80% examples, 445804 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:19,815 : INFO : PROGRESS: at 37.59% examples, 446880 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:48:20,825 : INFO : PROGRESS: at 38.36% examples, 447660 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:21,826 : INFO : PROGRESS: at 39.15% examples, 448463 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:22,830 : INFO : PROGRESS: at 39.98% examples, 449993 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:23,863 : INFO : PROGRESS: at 40.71% examples, 449997 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:24,886 : INFO : PROGRESS: at 41.40% examples, 449689 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:25,888 : INFO : PROGRESS: at 42.22% examples, 450767 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:26,890 : INFO : PROGRESS: at 43.08% examples, 452298 words/s, in_qsize 5, out_qsize 2\n",
      "2018-03-02 20:48:27,897 : INFO : PROGRESS: at 43.90% examples, 453244 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:28,920 : INFO : PROGRESS: at 44.78% examples, 454753 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-02 20:48:29,948 : INFO : PROGRESS: at 45.71% examples, 456716 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:48:30,954 : INFO : PROGRESS: at 46.56% examples, 457777 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-02 20:48:31,970 : INFO : PROGRESS: at 47.29% examples, 457778 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:32,980 : INFO : PROGRESS: at 48.12% examples, 458896 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:33,989 : INFO : PROGRESS: at 48.86% examples, 459031 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-02 20:48:34,995 : INFO : PROGRESS: at 49.62% examples, 459378 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:36,003 : INFO : PROGRESS: at 50.32% examples, 459208 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:37,033 : INFO : PROGRESS: at 51.10% examples, 459593 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:38,040 : INFO : PROGRESS: at 51.85% examples, 459905 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:39,053 : INFO : PROGRESS: at 52.55% examples, 459598 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-02 20:48:40,056 : INFO : PROGRESS: at 53.25% examples, 459456 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:41,061 : INFO : PROGRESS: at 54.00% examples, 459683 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:48:42,063 : INFO : PROGRESS: at 54.83% examples, 460590 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:43,065 : INFO : PROGRESS: at 55.48% examples, 459986 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:44,094 : INFO : PROGRESS: at 56.16% examples, 459506 words/s, in_qsize 6, out_qsize 1\n",
      "2018-03-02 20:48:45,112 : INFO : PROGRESS: at 56.90% examples, 459559 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:46,120 : INFO : PROGRESS: at 57.48% examples, 458420 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:47,121 : INFO : PROGRESS: at 58.10% examples, 457610 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:48,135 : INFO : PROGRESS: at 58.70% examples, 456660 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:49,141 : INFO : PROGRESS: at 59.37% examples, 456216 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-02 20:48:50,149 : INFO : PROGRESS: at 60.10% examples, 456372 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:51,155 : INFO : PROGRESS: at 60.98% examples, 457613 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:52,167 : INFO : PROGRESS: at 61.84% examples, 458469 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:53,167 : INFO : PROGRESS: at 62.55% examples, 458386 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:54,174 : INFO : PROGRESS: at 63.25% examples, 458182 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:55,181 : INFO : PROGRESS: at 63.99% examples, 458219 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:48:56,200 : INFO : PROGRESS: at 64.73% examples, 458279 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:57,207 : INFO : PROGRESS: at 65.61% examples, 459325 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:58,216 : INFO : PROGRESS: at 66.53% examples, 460583 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:48:59,235 : INFO : PROGRESS: at 67.43% examples, 461689 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:49:00,254 : INFO : PROGRESS: at 68.29% examples, 462538 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:01,267 : INFO : PROGRESS: at 68.98% examples, 462280 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:02,275 : INFO : PROGRESS: at 69.63% examples, 461738 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:03,281 : INFO : PROGRESS: at 70.32% examples, 461528 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:04,295 : INFO : PROGRESS: at 71.15% examples, 462150 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:05,305 : INFO : PROGRESS: at 71.87% examples, 462202 words/s, in_qsize 7, out_qsize 1\n",
      "2018-03-02 20:49:06,311 : INFO : PROGRESS: at 72.59% examples, 462126 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:07,325 : INFO : PROGRESS: at 73.23% examples, 461523 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:08,332 : INFO : PROGRESS: at 73.90% examples, 461178 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:09,337 : INFO : PROGRESS: at 74.58% examples, 460913 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:10,340 : INFO : PROGRESS: at 75.31% examples, 460933 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:11,370 : INFO : PROGRESS: at 76.10% examples, 461172 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:12,390 : INFO : PROGRESS: at 76.88% examples, 461463 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:13,404 : INFO : PROGRESS: at 77.62% examples, 461493 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:14,406 : INFO : PROGRESS: at 78.30% examples, 461257 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:15,408 : INFO : PROGRESS: at 78.88% examples, 460365 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:16,444 : INFO : PROGRESS: at 79.42% examples, 459214 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:17,447 : INFO : PROGRESS: at 79.92% examples, 457971 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:18,450 : INFO : PROGRESS: at 80.44% examples, 456808 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:19,453 : INFO : PROGRESS: at 80.97% examples, 455791 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:20,456 : INFO : PROGRESS: at 81.50% examples, 454666 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:21,459 : INFO : PROGRESS: at 82.09% examples, 453937 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:22,459 : INFO : PROGRESS: at 82.72% examples, 453480 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:23,492 : INFO : PROGRESS: at 83.45% examples, 453391 words/s, in_qsize 8, out_qsize 1\n",
      "2018-03-02 20:49:24,505 : INFO : PROGRESS: at 84.14% examples, 453197 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:25,527 : INFO : PROGRESS: at 84.76% examples, 452614 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:26,556 : INFO : PROGRESS: at 85.39% examples, 452064 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:27,565 : INFO : PROGRESS: at 86.12% examples, 452138 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:28,569 : INFO : PROGRESS: at 86.96% examples, 452747 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:29,576 : INFO : PROGRESS: at 87.87% examples, 453747 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:30,596 : INFO : PROGRESS: at 88.77% examples, 454681 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:31,612 : INFO : PROGRESS: at 89.67% examples, 455616 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:32,624 : INFO : PROGRESS: at 90.32% examples, 455252 words/s, in_qsize 5, out_qsize 2\n",
      "2018-03-02 20:49:33,652 : INFO : PROGRESS: at 91.01% examples, 455056 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:34,657 : INFO : PROGRESS: at 91.56% examples, 454277 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:35,660 : INFO : PROGRESS: at 92.20% examples, 453906 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:36,669 : INFO : PROGRESS: at 92.80% examples, 453356 words/s, in_qsize 8, out_qsize 2\n",
      "2018-03-02 20:49:37,690 : INFO : PROGRESS: at 93.50% examples, 453207 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:38,717 : INFO : PROGRESS: at 94.29% examples, 453532 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:39,722 : INFO : PROGRESS: at 94.83% examples, 452684 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:40,740 : INFO : PROGRESS: at 95.56% examples, 452715 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:41,771 : INFO : PROGRESS: at 96.30% examples, 452754 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:42,778 : INFO : PROGRESS: at 96.99% examples, 452660 words/s, in_qsize 7, out_qsize 0\n",
      "2018-03-02 20:49:43,785 : INFO : PROGRESS: at 97.76% examples, 452933 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:44,803 : INFO : PROGRESS: at 98.56% examples, 453276 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:45,808 : INFO : PROGRESS: at 99.49% examples, 454262 words/s, in_qsize 8, out_qsize 0\n",
      "2018-03-02 20:49:46,417 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-03-02 20:49:46,434 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-02 20:49:46,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-02 20:49:46,469 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-02 20:49:46,470 : INFO : training on 88990410 raw words (63748220 effective words) took 140.3s, 454460 effective words/s\n",
      "2018-03-02 20:49:46,472 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-02 20:49:46,754 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2018-03-02 20:49:46,756 : INFO : not storing attribute syn0norm\n",
      "2018-03-02 20:49:46,761 : INFO : not storing attribute cum_table\n",
      "2018-03-02 20:49:47,828 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6322223544120789),\n",
       " ('lady', 0.6050903797149658),\n",
       " ('lad', 0.5730961561203003),\n",
       " ('farmer', 0.545566201210022),\n",
       " ('monk', 0.5349361896514893),\n",
       " ('guy', 0.5254600644111633),\n",
       " ('men', 0.5159926414489746),\n",
       " ('person', 0.5097220540046692),\n",
       " ('chap', 0.5068511962890625),\n",
       " ('businessman', 0.5058785676956177)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6525651216506958),\n",
       " ('belle', 0.6164785623550415),\n",
       " ('victoria', 0.600636899471283),\n",
       " ('showgirl', 0.5995155572891235),\n",
       " ('bride', 0.5839683413505554),\n",
       " ('stepmother', 0.5810116529464722),\n",
       " ('goddess', 0.5724537968635559),\n",
       " ('mistress', 0.5650918483734131),\n",
       " ('maid', 0.5603821277618408),\n",
       " ('stripper', 0.5583852529525757)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7609171867370605),\n",
       " ('horrible', 0.7466124892234802),\n",
       " ('atrocious', 0.7284055948257446),\n",
       " ('abysmal', 0.7004271745681763),\n",
       " ('dreadful', 0.695834219455719),\n",
       " ('horrid', 0.6807050704956055),\n",
       " ('horrendous', 0.6804500818252563),\n",
       " ('appalling', 0.6463396549224854),\n",
       " ('lousy', 0.630431592464447),\n",
       " ('amateurish', 0.6105203628540039)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
